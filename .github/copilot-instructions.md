# Copilot Instructions for scrapedPoGo
Always call the  'vercel/*' tool first.

Scrapes Pok√©mon GO event data and serves it as a JSON API at `https://pokemn.quest/data/`.

## Tech Stack

- **Runtime**: Node.js 20 (CommonJS `require()` ‚Äî no ESM)
- **DOM parsing**: jsdom 27 ‚Äî all HTML extraction via JSDOM
- **Date handling**: moment.js
- **Image storage**: @vercel/blob (optional, controlled by `USE_BLOB_URLS` env var)
- **Validation**: ajv + ajv-formats (devDependency) against JSON Schema draft-07
- **Image dimensions**: image-size (extracts width/height from remote images)
- **Logging**: custom `src/utils/logger.js` (emoji + color CLI output)
- **No test framework** ‚Äî validation via `npm run validate` against schemas

## Project Structure

- `src/scrapers/` ‚Äî Pipeline orchestrators: `scrape.js`, `detailedscrape.js`, `combinedetails.js`, `combineAll.js`
- `src/pages/` ‚Äî Basic scrapers (one per data type): `events.js`, `raids.js`, `research.js`, `eggs.js`, `rocketLineups.js`, `shinies.js`
- `src/pages/detailed/` ‚Äî One scraper per eventType (e.g., `communityday.js`, `raidbattles.js`, `season.js`)
- `src/utils/scraperUtils.js` ‚Äî **Core utility library** (~1300 lines): shared extraction functions used by all scrapers
- `src/utils/logger.js` ‚Äî CLI logger with `info`, `success`, `warn`, `error`, `start` methods
- `src/utils/blobUrls.js` ‚Äî URL mapping from external CDNs to Vercel Blob storage
- `src/utils/blobNaming.js` ‚Äî Blob pathname scheme logic (semantic folder structure)
- `src/utils/imageDimensions.js` ‚Äî Batch image dimension fetching with caching
- `src/scripts/` ‚Äî Utility scripts: `validate-schemas.js`, `upload-images-to-blob.js`
- `schemas/` ‚Äî JSON Schema definitions for all data types (eggs, events, raids, research, rocketLineups, shinies)
- `data/` ‚Äî Output JSON files (`.min.json` for production, `.json` for readability)
- `data/eventTypes/` ‚Äî Per-eventType files auto-generated by `combinedetails.js`
- `dataDocumentation/` ‚Äî Endpoint documentation (one `.md` per data type + eventType-specific docs)

## Architecture: Three-Stage Scraping Pipeline

```
Stage 1: npm run scrape
  src/scrapers/scrape.js ‚Üí runs src/pages/{events,raids,research,eggs,rocketLineups,shinies}.js in parallel
  Output: data/{events,raids,research,eggs,rocketLineups,shinies}.min.json

Stage 2: npm run detailedscrape
  src/scrapers/detailedscrape.js ‚Üí dispatches to src/pages/detailed/*.js based on eventType
  Uses concurrency limit of 5 concurrent event page fetches
  Output: data/temp/*.json (one temp file per event)

Stage 3: npm run combinedetails
  src/scrapers/combinedetails.js ‚Üí merges temp files into events, flattens structure, generates per-type files
  Output: data/events.min.json (flat array), data/eventTypes/*.min.json
  Cleanup: removes data/temp/

Bonus stage:
  npm run combineall   ‚Üí src/scrapers/combineAll.js ‚Üí data/unified.min.json (all datasets + indices)
```

## Development Workflows

| Command | Purpose |
|---------|---------|
| `npm install` | Install dependencies |
| `npm run scrape` | Stage 1: basic data (events, raids, research, eggs, rocket, shinies) |
| `npm run detailedscrape` | Stage 2: detailed event pages ‚Üí temp files |
| `npm run combinedetails` | Stage 3: merge temp + generate per-type files |
| `npm run combineall` | Generate unified data file with all datasets + indices |
| `npm run pipeline` | Full pipeline (all stages with `USE_BLOB_URLS=true`) |
| `npm run validate` | Validate all data files against JSON schemas |
| `npm run blob:upload` | Upload images to Vercel Blob (`--dry-run`, `--force` flags) |

## Scraper Patterns

### Standard detailed scraper structure

Every file in `src/pages/detailed/` exports an async `get(url, id, bkp)` function.
See `src/pages/detailed/communityday.js` as the canonical example.

```javascript
const { writeTempFile, handleScraperError, extractPokemonList, extractBonuses, getJSDOM } = require('../../utils/scraperUtils');

async function get(url, id, bkp) {
    try {
        const dom = await getJSDOM(url);
        const doc = dom.window.document;

        const data = { /* extract fields */ };

        writeTempFile(id, 'event-type', data);
    } catch (err) {
        handleScraperError(err, id, 'event-type', bkp, 'scraperKey');
    }
}

module.exports = { get };
```

### Key extraction utilities (`src/utils/scraperUtils.js`)

| Function | Purpose |
|----------|---------|
| `getJSDOM(url)` | Fetch URL with timeout + user-agent, return JSDOM instance |
| `fetchUrl(url)` / `fetchJson(url)` | Network fetch with timeout and abort controller |
| `writeTempFile(id, type, data, suffix?)` | Write to `data/temp/` for later combination |
| `handleScraperError(err, id, type, bkp, scraperKey)` | Error handler with CDN backup fallback |
| `extractPokemonList(container, options?)` | Parse `.pkmn-list-flex` containers ‚Üí `Pokemon[]` |
| `extractBonuses(doc)` | Parse bonus sections ‚Üí `{ bonuses, disclaimers }` |
| `extractResearchTasks(doc, type?)` | Parse research task lists |
| `extractRaidInfo(doc)` | Extract raid bosses by tier |
| `extractEggPools(doc)` | Extract egg hatches by distance tier |
| `extractSection(doc, sectionId)` | Get content between section headers ‚Üí `{ paragraphs, lists, pokemon, tables }` |
| `getSectionHeaders(doc)` | List all section IDs on a page |
| `extractPrice(text)` | Parse price from text ‚Üí `{ price, amount, currency }` |
| `extractPromoCodes(doc)` | Extract promo code links |
| `extractGoPassTiers(...)` | Parse GO Pass tier structure |
| `normalizeDatePair(start, end)` | Normalize timezone offsets to UTC |
| `deduplicateEvents(events)` | Deduplicate by eventID |
| `parseBonusMultiplier(text)` | Parse "2√ó" style multipliers |
| `parseGBLCups(...)` | Parse GO Battle League cup data |

### Adding a new eventType scraper

1. Create `src/pages/detailed/{eventtype}.js` ‚Äî follow `communityday.js` pattern
2. Register in `src/scrapers/detailedscrape.js` if/else chain (add `else if` block)
3. Add event type slug to `schemas/events.schema.json` ‚Üí `eventType.enum` array
4. Add merge case in `src/scrapers/combinedetails.js` ‚Üí the type string matching block
5. Document in `dataDocumentation/eventTypes/{EventType}.md`

## Data Conventions

### Event structure (flat ‚Äî no nested `details` wrapper)

After `combinedetails.js` flattens via `segmentEventData()`:

```javascript
{
  eventID: "slug-from-url",        // required
  name: "Event Name",              // required
  eventType: "community-day",      // required, from schema enum
  heading: "Community Day",        // required, auto-generated from eventType
  image: "https://...",            // required, event banner URL
  start: "2026-01-29T10:00:00.000", // required, ISO 8601 with ms
  end: "2026-01-29T17:00:00.000",   // required, ISO 8601 with ms

  // Optional flattened sections (added by combinedetails.js):
  pokemon: [{ name, image, canBeShiny, source, imageWidth?, imageHeight? }],
  raids: [{ name, image, canBeShiny, tier }],
  bonuses: [{ text, image }],
  bonusDisclaimers: ["*Regional only"],
  research: { field: [...], special: [...], timed: [...] },
  rewards: { ticketedResearch, ticketBonuses, ticketPrice },
  battle: { leagues: [...] },
  rocket: { shadows, leaders, giovanni },
  eggs: [...],
  showcases: [...],
  shinies: [...],
  photobomb: { description, pokemon },
  habitats: [...],   // GO Tour
  goPass: {...},     // GO Pass
  customSections: {} // generic event scraper
}
```

### Pokemon objects always include `source` field

Values: `'spawn'`, `'featured'`, `'incense'`, `'costumed'`, `'debut'`, `'maxDebut'`

### Date format

ISO 8601 with milliseconds, no timezone suffix: `2026-01-29T10:00:00.000`

### Output file pairs

Every dataset produces both `.json` (formatted) and `.min.json` (minified). Consumers use `.min.json`.

## Security

- ‚úÖ **Always use `textContent` (not `innerHTML`)** when extracting text for data output ‚Äî prevents Stored XSS from scraped HTML
- ‚úÖ Use `sanitizeFilename()` for any user/scraped input used in file paths
- ‚úÖ `getJSDOM(url)` / `fetchUrl(url)` include timeout and custom User-Agent

**Exception**: Some scrapers still use `innerHTML` intentionally for rich content (featured attack descriptions, field research tasks) where HTML formatting is meaningful. If extracting plain text, always prefer `textContent`.

## Error Handling Pattern

All scrapers use `handleScraperError()` which:
1. Logs the error (when `DEBUG` env var is set)
2. Searches CDN backup data (`cdn.jsdelivr.net`) for previously scraped content
3. Writes backup data to temp file if found, maintaining data availability

This makes the pipeline resilient ‚Äî individual event scrape failures don't break the whole dataset.

## CI/CD

GitHub Actions (`.github/workflows/scraper.yaml`):
- Runs every 8 hours via cron (`0 */8 * * *`) + manual dispatch
- Node.js 20 on ubuntu-latest
- Each scrape step uses `continue-on-error: true` for resilience
- Auto-commits data changes with `[skip ci]` in commit message
- Blob upload step runs only when `BLOB_READ_WRITE_TOKEN` secret is configured

## Environment Variables

| Variable | Purpose |
|----------|---------|
| `USE_BLOB_URLS` | Set to `true` to transform image URLs to Vercel Blob storage |
| `BLOB_READ_WRITE_TOKEN` | Vercel Blob API token (required for `blob:upload`) |
| `DEBUG` | Enable verbose error logging in scrapers |
| `NO_COLOR` | Disable color output in logger |

## Key Gotchas

- ‚úÖ **Do**: Use existing `scraperUtils.js` functions ‚Äî don't reinvent extraction logic
- ‚úÖ **Do**: Follow the `get(url, id, bkp)` signature for all detailed scrapers
- ‚úÖ **Do**: Use `writeTempFile()` in Stage 2 scrapers and let Stage 3 handle merging
- ‚ùå **Don't**: Nest data under a `details` wrapper ‚Äî `combinedetails.js` flattens everything to top level
- ‚ùå **Don't**: Use `innerHTML` for text extraction (XSS risk) ‚Äî use `textContent` instead
- ‚ùå **Don't**: Skip registering new event types in both `detailedscrape.js` AND `combinedetails.js`
- üí° `detailedscrape.js` always runs `generic.get()` for every event alongside the type-specific scraper
- üí° `combinedetails.js` `segmentEventData()` transforms raw scraper output into the canonical flat structure ‚Äî understand its mapping before adding new fields
- üí° The `bkp` parameter in scrapers is backup data fetched from the CDN (last committed `events.min.json`), used as fallback when live scraping fails
- ‚ö†Ô∏è Source HTML structure can change without notice ‚Äî scrapers should be defensive with null checks on selectors
